import numpy as np
import matplotlib.pyplot as plt

from keras.layers import *
from keras.layers.advanced_activations import LeakyReLU
from keras.models import Sequential,Model
from keras.optimizers import Adam
from keras.datasets import mnist

import math
print(X_train.shape)

#Image Plotting
plt.imshow(X_train[1],cmap='gray')
plt.show()

# Normalising Data
X_train=(X_train.astype('float32')-127.5)/127.5 # Pixel values are between 0 and 255 so we took central value
print(np.max(X_train))
print(np.min(X_train))
print(X_train.shape)

# Parameter Defining
TOTAL_EPOCHS = 50
BATCH_SIZE = 256
NO_OF_BATCHES = int(X_train.shape[0]/BATCH_SIZE)
HALF_BATCH = 128
NOISE_DIM = 100 # It will get upscaled to 784
adam = Adam(lr=2e-4,beta_1=0.5) # Default Parameters generally works well for GANs
# Generator Building with input noise of dimension (100)
generator=Sequential()
generator.add(Dense(256,input_shape=(NOISE_DIM,)))
generator.add(LeakyReLU(0.2)) # 0.2 is alpha 
generator.add(Dense(512))
generator.add(LeakyReLU(0.2))
generator.add(Dense(1024))
generator.add(LeakyReLU(0.2))
generator.add(Dense(784,activation='tanh'))
generator.compile(loss='binary_crossentropy',optimizer=adam)
generator.summary()
# Discriminator Building
# We would downsample input of size 784 into 512
discriminator=Sequential()
discriminator.add(Dense(512,input_shape=(784,)))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dense(256))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dense(1,activation='sigmoid'))
discriminator.compile(loss='binary_crossentropy',optimizer='adam')
discriminator.summary()
# GAN Model(This one is actually step 2)
# Defining variables 
discriminator.trainable=False # Freeze discriminator
gan_input=Input(shape=(NOISE_DIM,))
generated_img=generator(gan_input)
gan_output=discriminator(generated_img)

#Functional APIs
model=Model(gan_input,gan_output)
model.compile(loss='binary_crossentropy',optimizer=adam)
X_train=X_train.reshape(-1,784)
print(X_train.shape)
def save_imgs(epoch,samples=100):
    
    noise = np.random.normal(0,1,size=(samples,NOISE_DIM))
    generated_imgs = generator.predict(noise)
    generated_imgs = generated_imgs.reshape(samples,28,28)
    
    plt.figure(figsize=(10,10))
    for i in range(samples):
        plt.subplot(10,10,i+1)
        plt.imshow(generated_imgs[i],interpolation='nearest',cmap='gray')
        plt.axis("off")
        
    plt.tight_layout()
    plt.savefig('images/gan_output_epoch_{0}.png'.format(epoch+1))
    plt.show()
    # Training of Step 1
for epoch in range(TOTAL_EPOCHS):
  epoch_d_loss=0.
  epoch_g_loss=0.

  # Mini Batch SGD
  for step in range(NO_OF_BATCHES):

    # Step 1 Discriminator
    # 50% Real Data + 50% Fake Data

    # Real Data X
    idx=np.random.randint(0,X_train.shape[0],HALF_BATCH) # Generating Random indexes
    real_imgs=X_train[idx] # Corresponding images

    # Fake Data X
    noise=np.random.normal(0,1,size=(HALF_BATCH,NOISE_DIM))
    fake_imgs=generator.predict(noise)

    # Labels
    real_y=np.ones((HALF_BATCH,1))*0.9 # For better results instead of 1 we take 0.9 in Gans
    fake_y=np.zeros((HALF_BATCH,1))

    # Training our Discriminator
    d_loss_real = discriminator.train_on_batch(real_imgs,real_y)
    d_loss_fake = discriminator.train_on_batch(fake_imgs,fake_y)
    d_loss=0.5*d_loss_real+0.5*d_loss_fake

    epoch_d_loss+=d_loss

    # Train Generator(Considering Frozen Discriminator)
    noise=np.random.normal(0,1,size=(BATCH_SIZE,NOISE_DIM))
    ground_truth_y=np.ones((BATCH_SIZE,1))
    g_loss=model.train_on_batch(noise,ground_truth_y)
    epoch_g_loss+=g_loss
  print("Epoch %d Disc Loss %.4f Generator Loss %.4f" %((epoch+1),epoch_d_loss/NO_OF_BATCHES,epoch_g_loss/NO_OF_BATCHES))
  #d_losses.append(epoch_d_loss/NO_OF_BATCHES)
  #g_losses.append(epoch_g_loss/NO_OF_BATCHES)
  if (epoch+1)%5==0:
    generator.save('model/gan_generator_{0}.h5'.format(epoch+1))
    save_imgs(epoch)
    
